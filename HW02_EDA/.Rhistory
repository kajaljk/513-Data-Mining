################################################
#  Purpose    : Assignment 3 knn
#  First Name : Kajal
#  Last Name  : Khunt
#  Id			    : 10429885
#################################################
rm(list = ls())
library(class)
## 3.1 Using knn (k=2), EXCEL and the following training and test datasets, predict the species for the test datasets.
## What is the error rate? DO NOT normalize the data. You can copy and paste the tables into excel.
dsn <- read.csv("D:\\MS\\Spring 2018\\CS-513\\Assignment\\HW03_knn\\training-test.csv")
#View(dsn)
test_idx<-1:3
Test_data<-dsn[test_idx,]
Training_data<-dsn[-test_idx,]
knn_result<-knn(Training_data[,-5],Test_data[,-5],Training_data[,5],k=2)
table(Prediction = knn_result,Actual=Test_data[,5])
Error_Rate <- mean(knn_result != Test_data[,5])
Error_Rate
## 3.2 Load the "breast-cancer-wisconsin.data.csv" from CANVAS (see the description bellow)
rm(list = ls())
dsn <- read.csv("D:\\MS\\Spring 2018\\CS-513\\Assignment\\HW03_knn\\breast-cancer-wisconsin.data.csv")
#View(dsn)
## a.  Remove the rows with missing values
dsn[dsn=='?']<-NA
dsn_na<-na.omit(dsn)
nrow(dsn_na)
## b.  Store every fifth record in a "test" dataset starting with the first record
test_idx<-seq(1,nrow(dsn_na),by=5)
Test_data<-dsn_na[test_idx,]
## c.   Store the rest in the "training" dataset
Training_data<-dsn_na[-test_idx,]
## d.  Use knn with k=1 and classify the test dataset
knn_result<-knn(Training_data[,-11],Test_data[,-11],Training_data[,11],k=1)
table(Prediction = knn_result,Actual=Test_data[,11])
##e
wrong<-(test[,11]!=predict)
rate<-sum(wrong)/length(wrong)
rate
## f.   Repeat the above steps with k=2, k=5, k=10.
## k = 2
knn_result<-knn(Training_data[,-11],Test_data[,-11],Training_data[,11],k=2)
table(Prediction = knn_result,Actual=Test_data[,11])
##knn_result<-knn(Training_data[,c(-1,-11)],Test_data[,-c(-1,11)],Training_data[,11],k=2)
## k = 5
knn_result<-knn(Training_data[,-11],Test_data[,-11],Training_data[,11],k=5)
table(Prediction = knn_result,Actual=Test_data[,11])
## k = 10
knn_result<-knn(Training_data[,-11],Test_data[,-11],Training_data[,11],k=10)
table(Prediction = knn_result,Actual=Test_data[,11])
rm(list = ls())
dsn <- read.csv("D://MS/Spring 2018/CS-513/Assignment/HW02_EDA/breast-cancer-wisconsin.data.csv")
View(dsn)
summary(dsn)
dsn$F6[dsn$F6=='?']<-NA
dsn$F6<-as.numeric(dsn$F6)
dsn$F6
sum(is.na(dsn))
library(modeest)
mfv <- mlv(dsn$F6,method = "mfv",na.rm=TRUE)
mfv$M
dsn[is.na(dsn$F6),"F6"]<-mfv$M
summary(dsn)
table(class=dsn$Class,F6=dsn$F6)
rm(list=ls())
installed.packages()
library(kknn)
library(kknn)
mmnorm <-function(x,minx,maxx) {z<-((x-minx)/(maxx-minx))
return(z)
}
iris_normalized<-as.data.frame (
cbind( Sepal.Length=mmnorm(iris[,1],min(iris[,1]),max(iris[,1]))
, sepal.Width=mmnorm(iris[,2],min(iris[,2]),max(iris[,2] ))
,Petal.Length=mmnorm(iris[,3],min(iris[,3]),max(iris[,3] ))
, Petal.Width=mmnorm(iris[,4],min(iris[,4]),max(iris[,4] ))
,Species=as.character(iris[,5])
)
)
index <- seq(1,nrow(iris_normalized ),by=5)
View(iris)
index
test<-iris_normalized[index,]
training <-iris_normalized[-index,]
View(test)
predict_k5 <- kknn(formula=Species~., training, test, k=5,kernel ="triangular" )
fit <- fitted(predict_k5)
?kknn()
library(rpart)
install.packages("rpart")
install.packages("rpart")
library(rpart)
dsn<-read.csv("D:\\MS\\Spring 2018\\CS-513\\Class Notes\\Week 7\\Titanic_rows.csv")
set.seed(123)
index<-sort(sample(nrow(dsn),round(.25*nrow(dsn))))
training<-dsn[-index,]
test<-dsn[index,]
CART_class<-rpart(Survived~.,data=training)
rpart.plot(CART_class)
install.packages("rpart.plot")
library(rpart.plot)  			# Enhanced tree plots
install.packages("rattle")
install.packages("RColorBrewer")
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)
CART_class<-rpart(Survived~.,data=training)
rpart.plot(CART_class)
CART_predict<-predict(CART_class,test)
str(CART_predict)
CART_class
CART_predict
#########################################################
##  Purpose: Create pretty classification tree
##  Developer: KD
##
#########################################################
#########################################################
##  Step 0: Clear the environment
##
##
#########################################################
rm(list=ls())
#########################################################
##  Step 1: Load the relavent packages
##
##
#########################################################
installed.packages()
#install.packages("rpart")  # CART standard package
?install.packages()
#install.packages("rpart")
#install.packages("rpart.plot")     # Enhanced tree plots
#install.packages("rattle")         # Fancy tree plot
#install.packages("RColorBrewer")   # colors needed for rattle
library(rpart)
library(rpart.plot)  			# Enhanced tree plots
library(rattle)           # Fancy tree plot
library(RColorBrewer)     # colors needed for rattle
dsn<-read.csv("D:\\MS\\Spring 2018\\CS-513\\Class Notes\\Week 7\\Titanic_rows.csv")
set.seed(123)
?ifelse
index<-sort(sample(nrow(dsn),round(.25*nrow(dsn))))
training<-dsn[-index,]
test<-dsn[index,]
table(dsn$survied)
?rpart()
#Grow thw tree
CART_class<-rpart(Survived~.,data=training)
rpart.plot(CART_class)
CART_predict<-predict(CART_class,test)
str(CART_predict)
CART_predict_Cat<-ifelse(CART_predict[,1]<=5,'Yes','No')
table(dsn$Survived,dsn$Sex)
## a better plot
library(rpart.plot)
prp(mytree)
# a much fancier graph
fancyRpartPlot(mytree)
CART_predict
CART_predict_Cat<-ifelse(CART_predict[,1]<=.5,'Yes','No')
CART_predict_Cat
table(Actual=test[,4],CART=CART_predict_Cat)
CART_wrong<-sum(test[,4]!=CART_predict_Cat)
CART_error_rate<-CART_wrong/length(test[,4])
CART_error_rate
CART_predict2<-predict(CART_class,test,type="class")
CART_wrong2<-sum(test[,4]!=CART_predict2)
CART_error_rate2<-CART_wrong2/length(test[,4])
CART_error_rate2
rpart.plot(CART_class)
install.packages("randomforest")
library(randomforest)
install.packages("randomforest")
install.packages("randomForest")
library(randomForest)
dsn<-read.csv("D:\\MS\\Spring 2018\\CS-513\\Class Notes\\Week 7\\Titanic_rows.csv")
set.seed(123)
